[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "PIC 16B - 2023 Fall Blog"
  },
  {
    "objectID": "posts/quarto/index.html",
    "href": "posts/quarto/index.html",
    "title": "Hello, Quarto",
    "section": "",
    "text": "In this post, we’ll get set up with Quarto.\nQuarto is a static site converter, which you can use to turn plaintext documents into attractive webpages. You should have already installed Quarto and signed up for Quarto Pub when completing the software installation (details in BruinLearn)."
  },
  {
    "objectID": "posts/quarto/index.html#make-a-blog",
    "href": "posts/quarto/index.html#make-a-blog",
    "title": "Hello, Quarto",
    "section": "Make a blog",
    "text": "Make a blog\nThis should be very straightforward using the instructions at this link.\nAll you need to do is run the following code at the terminal.\nquarto create-project myblog --type website:blog\nThen once a folder appears, run this line.\nquarto preview myblog\nFeel free to replace myblog with a different name.\nquarto preview should open up a link that looks like http://localhost:6832 in your web browser. The port number probably looks different."
  },
  {
    "objectID": "posts/quarto/index.html#publish-on-quarto-pub",
    "href": "posts/quarto/index.html#publish-on-quarto-pub",
    "title": "Hello, Quarto",
    "section": "Publish on Quarto Pub",
    "text": "Publish on Quarto Pub\nNow, run this line on your terminal in the same directory.\nquarto publish myblog\nWhen prompted, select these options.\n? Provider: › Quarto Pub\n? Authorize (Y/n) › Yes\nFinally, the terminal should print out something like this:\n[✓] Creating quarto-pub site\n[✓] Preparing to publish site\n[✓] Uploading files (complete)\n[✓] Deploying published site\n[✓] Published site: https://quartopub.com/sites/[...]\n[✓] Account site updated: https://[Quarto Pub username].quarto.pub\nGo to the website on the last line, and if you see a webpage there, congrats! Your blog is up and running. At the moment, it’s just a copy of the template, so it’s not personalized in any way."
  },
  {
    "objectID": "posts/quarto/index.html#edit-a-post",
    "href": "posts/quarto/index.html#edit-a-post",
    "title": "Hello, Quarto",
    "section": "Edit a post",
    "text": "Edit a post\nPreview your blog again:\nquarto preview myblog\nThen edit the welcome page in posts/welcome/index.qmd. Any sort of change will do.\nOnce you save the file, you’ll see that the preview page on the web browser is automatically updated.\nYou can also add a new page following instructions in this post."
  },
  {
    "objectID": "posts/quarto/index.html#publish-again",
    "href": "posts/quarto/index.html#publish-again",
    "title": "Hello, Quarto",
    "section": "Publish again",
    "text": "Publish again\nOnce you’ve made all these additions, publish the result again using quarto publish. In a few minutes, you should see your new post on your website."
  },
  {
    "objectID": "posts/hw0/index.html",
    "href": "posts/hw0/index.html",
    "title": "Homework 0",
    "section": "",
    "text": "In this blog post assignment, you’ll create a short post for your new website. The primary purpose is to give you some practice working with Quarto blogging with Python code.\nMake sure to check the “Specifications” section at the bottom of this assignment for an explicit list of criteria that your blog post must meet in order to receive credit."
  },
  {
    "objectID": "posts/hw0/index.html#complete-the-hello-quarto-activity",
    "href": "posts/hw0/index.html#complete-the-hello-quarto-activity",
    "title": "Homework 0",
    "section": "1. Complete the Hello, Quarto activity",
    "text": "1. Complete the Hello, Quarto activity\nYour first step should be to complete the Hello Quarto activity to help you get familiar with blogging with Quarto. If you already completed this activity in Discussion, then you can skip to the next step.\nIf you haven’t done so already, now is a good time to modify your site. Look around the site’s files and see if you can figure out how to modify the About page and the blog’s title from about.qmd and _quarto.yml.\nSee these pages for help: about page, config options.\nIf you are comfortable with css, then you can directly modify style.css and other files in the repo.\nAll this is optional, and it’s not necessary to put your real name or real photo anywhere on the site."
  },
  {
    "objectID": "posts/hw0/index.html#create-a-post",
    "href": "posts/hw0/index.html#create-a-post",
    "title": "Homework 0",
    "section": "2. Create a post",
    "text": "2. Create a post\nCreate a simple blog post, using the instructions and demonstrations here. Here is the prompt for your post:\n\nWrite a tutorial explaining how to construct an interesting data visualization of the Palmer Penguins data set.\n\nYou can read the data into Python by running:\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\nYour visualization does not have to be complex or fancy, but it should be highly readable and appropriately labeled.\nYour post should include the image directly under the code that generates it, as demonstrated here.\nThere will be two Gradescope assignments open for submission, one for PDF, and the other for code portion (as a programming assignment). You have to submit both of them for your homework to be graded.\n\nFor the PDF assingment, please submit your newly-created blog page printed as PDF. You do not need to publish your blog to the web for the homework. It is enough to print the preview page as a PDF and submit on Gradescope. If you want to publish it online, you are welcome to do so.\nFor the programming assignment, please submit any code file you wrote for your homework. All the .py file, .ipynb file, or .qmd files all included. The grader should be able to reproduce your result from the code portion you submitted.\n\n\nHint\nThe easiest way to create a post like this is to solve the problem in a Jupyter Notebook or Python script first, and then transfer the results over to your blog."
  },
  {
    "objectID": "posts/hw0/index.html#format",
    "href": "posts/hw0/index.html#format",
    "title": "Homework 0",
    "section": "Format",
    "text": "Format\n\nYou have to submit the PDF-printed version of your Quarto blog. Anything else will receive “In Progress” grade at best, incuding:\n\n\nJupyter notebook or JupyterLab screen printed\nHTML-converted Jupyter notebook printed\nPDF-converted Jupyter notebook or JupyterLab\nPDF generated directly from Quarto\n\n\nYou have to submit code necessary to reproduce your results."
  },
  {
    "objectID": "posts/hw0/index.html#coding-problem",
    "href": "posts/hw0/index.html#coding-problem",
    "title": "Homework 0",
    "section": "Coding Problem",
    "text": "Coding Problem\n\nThe plot is readable and contains axis labels, a title, and a legend if appropriate."
  },
  {
    "objectID": "posts/hw0/index.html#style-and-documentation",
    "href": "posts/hw0/index.html#style-and-documentation",
    "title": "Homework 0",
    "section": "Style and Documentation",
    "text": "Style and Documentation\n\nRepeated operations should be enclosed in functions.\nFor-loops are minimized by making full use of vectorized operations for Numpy arrays and Pandas data frames.\nHelpful comments are supplied throughout the code. Docstrings are supplied for any functions and classes you define."
  },
  {
    "objectID": "posts/hw0/index.html#writing",
    "href": "posts/hw0/index.html#writing",
    "title": "Homework 0",
    "section": "Writing",
    "text": "Writing\n\nThe overall post is written in engaging and unambiguous English prose. There is written explanation throughout the post, such that a PIC16A student could learn to perform the demonstrated tasks by reading the post.\nEach block of code has a clearly-explained purpose.\nThe post is organized into clearly delimited sections using markdown headers (#), making it easier for the reader to navigate."
  },
  {
    "objectID": "posts/hw4/index.html",
    "href": "posts/hw4/index.html",
    "title": "Homework 4: Fake News Classification",
    "section": "",
    "text": "Rampant misinformation — often called “fake news” — is one of the defining features of contemporary democratic life. In this Blog Post, you will develop and assess a fake news classifier using Tensorflow.\nNote: Working on this Blog Post in Google Colab is highly recommended."
  },
  {
    "objectID": "posts/hw4/index.html#data-source",
    "href": "posts/hw4/index.html#data-source",
    "title": "Homework 4: Fake News Classification",
    "section": "Data Source",
    "text": "Data Source\nOur data for this assignment comes from the article\n\nAhmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).\n\nI accessed it from Kaggle. I have done a small amount of data cleaning for you already, and performed a train-test split."
  },
  {
    "objectID": "posts/hw4/index.html#acquire-training-data",
    "href": "posts/hw4/index.html#acquire-training-data",
    "title": "Homework 4: Fake News Classification",
    "section": "1. Acquire Training Data",
    "text": "1. Acquire Training Data\nI have hosted a training data set at the below URL. You can either read it into Python directly (via pd.read_csv()) or download it to your computer and read it from disk.\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\nEach row of the data corresponds to an article. The title column gives the title of the article, while the text column gives the full article text. The final column, called fake, is 0 if the article is true and 1 if the article contains fake news, as determined by the authors of the paper above."
  },
  {
    "objectID": "posts/hw4/index.html#make-a-dataset",
    "href": "posts/hw4/index.html#make-a-dataset",
    "title": "Homework 4: Fake News Classification",
    "section": "2. Make a Dataset",
    "text": "2. Make a Dataset\nWrite a function called make_dataset. This function should do two things:\n\nRemove stopwords from the article text and title. A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.” You may find this StackOverFlow thread to be helpful.\nConstruct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (title, text), and the output should consist only of the fake column. You may find it helpful to consult lecture notes or this tutorial for reference on how to construct and use Datasets with multiple inputs.\n\nCall the function make_dataset on your training dataframe to produce a Dataset. You may wish to batch your Dataset prior to returning it, which can be done like this: my_data_set.batch(100). Batching causes your model to train on chunks of data rather than individual rows. This can sometimes reduce accuracy, but can also greatly increase the speed of training. Finding a balance is key. I found batches of 100 rows to work well.\n\nValidation Data\nAfter you’ve constructed your primary Dataset, split of 20% of it to use for validation.\n\n\nBase Rate\nRecall that the base rate refers to the accuracy of a model that always makes the same guess (for example, such a model might always say “fake news!”). Determine the base rate for this data set by examining the labels on the training set.\n\n\nTextVectorization\nHere is one option:\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation \n\ntitle_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500) \n\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))"
  },
  {
    "objectID": "posts/hw4/index.html#create-models",
    "href": "posts/hw4/index.html#create-models",
    "title": "Homework 4: Fake News Classification",
    "section": "3. Create Models",
    "text": "3. Create Models\nPlease use TensorFlow models to offer a perspective on the following question:\n\nWhen detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?\n\nTo address this question, create three (3) TensorFlow models.\n\nIn the first model, you should use only the article title as an input.\nIn the second model, you should use only the article text as an input.\nIn the third model, you should use both the article title and the article text as input.\n\nTrain your models on the training data until they appear to be “fully” trained. Assess and compare their performance. Make sure to include a visualization of the training histories.\nYou can visualize your models with this code:\nfrom tensorflow.keras import utils\nutils.plot_model(model)\n\nNotes\n\nFor the first two models, you don’t have to create new Datasets. Instead, just specify the inputs to the keras.Model appropriately, and TensorFlow will automatically ignore the unused inputs in the Dataset.\nThe lecture notes and tutorials linked above are likely to be helpful as you are creating your models as well.\nYou will need to use the Functional API, rather than the Sequential API, for this modeling task.\nWhen using the Functional API, it is possible to use the same layer in multiple parts of your model; see this tutorial for examples. I recommended that you share an embedding layer for both the article title and text inputs.\nYou may encounter overfitting, in which case Dropout layers can help.\n\nYou’re free to be creative when designing your models. If you’re feeling very stuck, start with some of the pipelines for processing text that we’ve seen in lecture, and iterate from there. Please include in your discussion some of the things that you tried and how you determined the models you used.\n\n\nWhat Accuracy Should You Aim For?\nYour three different models might have noticeably different performance. Your best model should be able to consistently score at least 97% validation accuracy.\nAfter comparing the performance of each model on validation data, make a recommendation regarding the question at the beginning of this section. Should algorithms use the title, the text, or both when seeking to detect fake news?"
  },
  {
    "objectID": "posts/hw4/index.html#model-evaluation",
    "href": "posts/hw4/index.html#model-evaluation",
    "title": "Homework 4: Fake News Classification",
    "section": "4. Model Evaluation",
    "text": "4. Model Evaluation\nNow we’ll test your model performance on unseen test data. For this part, you can focus on your best model, and ignore the other two.\nOnce you’re satisfied with your best model’s performance on validation data, download the test data here:\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\nYou’ll need to convert this data using the make_dataset function you defined in Part §2. Then, evaluate your model on the data. If we used your model as a fake news detector, how often would we be right?"
  },
  {
    "objectID": "posts/hw4/index.html#embedding-visualization",
    "href": "posts/hw4/index.html#embedding-visualization",
    "title": "Homework 4: Fake News Classification",
    "section": "5. Embedding Visualization",
    "text": "5. Embedding Visualization\nVisualize and comment on the embedding that your model learned (you did use an embedding, right?). Are you able to find any interesting patterns or associations in the words that the model found useful when distinguishing real news from fake news? You are welcome to use either 2-dimensional or 3-dimensional embedding. Comment on at least 5 words whose location in the embedding you find interpretable.\nI’d suggest that you create an embedding in a relatively large number of dimensions (say, 10) and then use PCA to reduce the dimension down to a visualizable number. This procedure was demonstrated in lecture."
  },
  {
    "objectID": "posts/hw4/index.html#format",
    "href": "posts/hw4/index.html#format",
    "title": "Homework 4: Fake News Classification",
    "section": "Format",
    "text": "Format\n\nAs always, please submit the PDF printout of your blog post and any code you wrote."
  },
  {
    "objectID": "posts/hw4/index.html#coding-problem",
    "href": "posts/hw4/index.html#coding-problem",
    "title": "Homework 4: Fake News Classification",
    "section": "Coding Problem",
    "text": "Coding Problem\n\nData Prep\n\nStopwords are removed during the construction of the data set.\nmake_dataset is implemented as a function, and used to create both the training/validation and testing data sets.\nThe constructed Dataset has multiple inputs.\n20% of the training data is split off for validation.\nThere is a comment on the base rate for the data set.\n\n\n\nModels\n\nModel 1 uses only the article title.\nModel 2 uses only the article text.\nModel 3 uses both the article title and text.\nVectorization is consistent across title and text for Model 3.\nThe training history is shown for each of the three models, including the training and validation performance.\nThe most performant model is evaluated on the test data set.\nThe best model consistently obtains at least 97% accuracy on the validation set.\nThe best model’s performance on the test set is shown.\n\n\n\nEmbedding Visualization\n\nA visualization of the learned word embedding is shown.\nThe written text discusses at least 5 words whose location is interpretable within the embedding.\n\n\n\nStyle and Documentation\n\nCode throughout is written using minimal repetition and clean style.\nDocstrings are not required in this Blog Post, but please make sure to include useful comments and detailed explanations for each of your code blocks.\nAny repeated operations should be enclosed in functions, regardless of whether they are explicitly required in the instructions.\n\n\n\nWriting\n\nThe blog post is written in tutorial format, in engaging and clear English. Grammar and spelling errors are acceptable within reason."
  },
  {
    "objectID": "posts/hw2/index.html",
    "href": "posts/hw2/index.html",
    "title": "Homework 2: Web Scraping",
    "section": "",
    "text": "What’s your favorite movie or TV show? Wouldn’t it be nice to find more shows that you might like to watch, based on ones you know you like? Tools that address questions like this are often called “recommender systems.” Powerful, scalable recommender systems are behind many modern entertainment and streaming services, such as Netflix and Spotify. While most recommender systems these days involve machine learning, there are also ways to make recommendations that don’t require such complex tools.\nIn this Blog Post, you’ll use webscraping to answer the following question:\nThe idea of this question is that, if TV show Y has many of the same actors as TV show X, and you like X, you might also enjoy Y.\nThis post has two parts. In the first, larger part, you’ll write a webscraper for finding shared actors on TMDB. In the second, smaller part, you’ll use the results from your scraper to make recommendations.\nDon’t forget to check the Specifications for a complete list of what you need to do to obtain full credit. As usual, this Blog Post should be printed as PDF from your PIC16B Blog preview screen, and you need to submit any code you wrote as well."
  },
  {
    "objectID": "posts/hw2/index.html#setup",
    "href": "posts/hw2/index.html#setup",
    "title": "Homework 2: Web Scraping",
    "section": "1. Setup",
    "text": "1. Setup\n\n1.1. Locate the Starting TMDB Page\nPick your favorite movie or TV show, and locate its TMDB page by searching on https://www.themoviedb.org/. For example, my favorite movie is Harry Potter and the Sorcerer’sPhilosopher’s Stone. Its TMDB page is at:\nhttps://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\nSave this URL for a moment.\n\n\n1.2. Dry-Run Navigation\nNow, we’re just going to practice clicking through the navigation steps that our scraper will take.\nFirst, click on the Full Cast & Crew link. This will take you to a page with URL of the form\n&lt;original_url&gt;cast/\nNext, scroll until you see the Cast section. Click on the portrait of one of the actors. This will take you to a page with a different-looking URL. For example, the URL for Alan Rickman, who played Severus Snape, is\nhttps://www.themoviedb.org/person/4566-alan-rickman\nFinally, scroll down until you see the actor’s Acting section. Note the titles of a few movies and TV shows in this section.\nOur scraper is going to replicate this process. Starting with your favorite movie or TV show, it’s going to look at all the actors in that movie or TV show, and then log all the other movies or TV shows that they worked on.\nAt this point, it would be a good idea for you to use the Developer Tools on your browser to inspect individual HTML elements and look for patterns among the names you are looking for.\n\n\n1.3. Initialize Your Project\n\nOpen a terminal and type:\n\nconda activate PIC16B-2\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nThis will create quite a lot of files, but you don’t really need to touch most of them. However, you must submit the entire TMDB_scraper folder for the code part.\n\n\n1.4. Tweak Settings\nFor now, add the following line to the file settings.py:\nCLOSESPIDER_PAGECOUNT = 20\nThis line just prevents your scraper from downloading too much data while you’re still testing things out. You’ll remove this line later.\nHint: Later on, you may run into 403 Forbidden errors once the website detects that you’re a bot. See these links (link1, link2, link3, link4) for how to work around that issue. The easiest solution is changing one line in setting.py. You might see this when you run scrapy shell as well, so keep an eye out for 403! Remember, you want your status to be 200 OK. If they know that you are on Python, they will certainly try to block you. One way to change user agent on scrapy shell is:\nscrapy shell -s USER_AGENT='Scrapy/2.8.0 (+https://scrapy.org)' https://www.themoviedb.org/..."
  },
  {
    "objectID": "posts/hw2/index.html#write-your-scraper",
    "href": "posts/hw2/index.html#write-your-scraper",
    "title": "Homework 2: Web Scraping",
    "section": "2. Write Your Scraper",
    "text": "2. Write Your Scraper\nCreate a file inside the spiders directory called tmdb_spider.py. Add the following lines to the file:\n# to run \n# scrapy crawl tmdb_spider -o movies.csv\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    \n    start_urls = ['https://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/']\nReplace the entry of start_urls with the URL corresponding to your favorite movie or TV show.\nNow, implement three parsing methods for the TmdbSpider class.\n\nparse(self, response) should assume that you start on a movie page, and then navigate to the Full Cast & Crew page. Remember that this page has url &lt;movie_url&gt;cast. (You are allowed to hardcode that part.) Once there, the parse_full_credits(self,response) should be called, by specifying this method in the callback argument to a yielded scrapy.Request. The parse() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\nparse_full_credits(self, response) should assume that you start on the Full Cast & Crew page. Its purpose is to yield a scrapy.Request for the page of each actor listed on the page. Crew members are not included. The yielded request should specify the method parse_actor_page(self, response) should be called when the actor’s page is reached. The parse_full_credits() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\nparse_actor_page(self, response) should assume that you start on the page of an actor. It should yield a dictionary with two key-value pairs, of the form {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name}. The method should yield one such dictionary for each of the movies or TV shows on which that actor has worked. Note that you will need to determine both the name of the actor and the name of each movie or TV show. This method should be no more than 15 lines of code, excluding comments and docstrings.\n\nProvided that these methods are correctly implemented, you can run the command\nscrapy crawl tmdb_spider -o results.csv\nto create a .csv file with a column for actors and a column for movies or TV shows.\nExperimentation in the scrapy shell is strongly recommended.\n\nChallenge\nIf you’re looking for a challenge, think about ways that may make your recommendations more accurate. Consider scraping the number of episodes as well or limiting the number of actors you get per show to make sure you only get the main series cast."
  },
  {
    "objectID": "posts/hw2/index.html#make-your-recommendations",
    "href": "posts/hw2/index.html#make-your-recommendations",
    "title": "Homework 2: Web Scraping",
    "section": "3. Make Your Recommendations",
    "text": "3. Make Your Recommendations\nOnce your spider is fully written, comment out the line\nCLOSESPIDER_PAGECOUNT = 20\nin the settings.py file. Then, the command\nscrapy crawl tmdb_spider -o results.csv\nwill run your spider and save a CSV file called results.csv, with columns for actor names and the movies and TV shows on which they worked.\nOnce you’re happy with the operation of your spider, compute a sorted list with the top movies and TV shows that share actors with your favorite movie or TV show. For example, it may have two columns: one for “movie names” and “number of shared actors”.\nFeel free to be creative. You can show a pandas data frame, a chart using matplotlib or plotly, or any other sensible display of the results."
  },
  {
    "objectID": "posts/hw2/index.html#blog-post",
    "href": "posts/hw2/index.html#blog-post",
    "title": "Homework 2: Web Scraping",
    "section": "4. Blog Post",
    "text": "4. Blog Post\nIn your blog post, you should describe how your scraper works, as well as the results of your analysis. When describing your scraper, I recommend dividing it up into the three distinct parsing methods, and discussing them one-by-one. For example:\n\nIn this blog post, I’m going to make a super cool web scraper… Here’s a link to my project repository… Here’s how we set up the project…\n&lt;implementation of parse()&gt;\nThis method works by…\n\n\n&lt;implementation of parse_full_credits()&gt;\nTo write this method, I…\n\nIn addition to describing your scraper, your Blog Post should include a table or visualization of numbers of shared actors.\nRemember that this post is still a tutorial, in which you guide your reader through the process of setting up and running the scraper. Don’t forget to tell them how to create the project and run the scraper!"
  },
  {
    "objectID": "posts/hw2/index.html#format",
    "href": "posts/hw2/index.html#format",
    "title": "Homework 2: Web Scraping",
    "section": "Format",
    "text": "Format\n\nYou have to submit the PDF-printed version of your Quarto blog. Anything else will receive “In Progress” grade at best, incuding:\n\n\nJupyter notebook or JupyterLab screen printed\nHTML-converted Jupyter notebook printed\nPDF-converted Jupyter notebook or JupyterLab\nPDF generated directly from Quarto\n\n\nYou have to submit code necessary to reproduce your results."
  },
  {
    "objectID": "posts/hw2/index.html#coding-problem",
    "href": "posts/hw2/index.html#coding-problem",
    "title": "Homework 2: Web Scraping",
    "section": "Coding Problem",
    "text": "Coding Problem\n\nEach of the three parsing methods appear logically and correctly implemented.\nparse() is implemented in no more than 5 lines.\nparse_full_credits() is implemented in no more than 5 lines.\nparse_actor_page() is implemented in no more than 15 lines.\nA table or list of results or pandas dataframe is shown.\nA visualization with matplotlib, plotly, or seaborn is shown."
  },
  {
    "objectID": "posts/hw2/index.html#style-and-documentation",
    "href": "posts/hw2/index.html#style-and-documentation",
    "title": "Homework 2: Web Scraping",
    "section": "Style and Documentation",
    "text": "Style and Documentation\n\nEach of the three parse methods has a short docstring describing its assumptions (e.g. what kind of page it is meant to parse) and its effect, including navigation and data outputs.\nEach of the three parse methods has helpful comments for understanding how each chunk of code operates."
  },
  {
    "objectID": "posts/hw2/index.html#writing",
    "href": "posts/hw2/index.html#writing",
    "title": "Homework 2: Web Scraping",
    "section": "Writing",
    "text": "Writing\n\nThe blog post is written in tutorial format, in engaging and clear English. Grammar and spelling errors are acceptable within reason.\nThe blog post explains clearly how to set up the project, run the scraper, and access the results.\nThe blog post explains how each of the three parse methods works.\nBlog post has a descriptive title."
  },
  {
    "objectID": "posts/hw3/index.html",
    "href": "posts/hw3/index.html",
    "title": "Homework 3: Image Classification",
    "section": "",
    "text": "In this blog post, you will learn several new skills and concepts related to image classification in TensorFlow."
  },
  {
    "objectID": "posts/hw3/index.html#acknowledgment",
    "href": "posts/hw3/index.html#acknowledgment",
    "title": "Homework 3: Image Classification",
    "section": "Acknowledgment",
    "text": "Acknowledgment\nMajor parts of this Blog Post assignment, including several code chunks, are based on the TensorFlow Transfer Learning Tutorial. You may find that consulting this tutorial is helpful while completing this assignment, although this shouldn’t be necessary."
  },
  {
    "objectID": "posts/hw3/index.html#load-packages-and-obtain-data",
    "href": "posts/hw3/index.html#load-packages-and-obtain-data",
    "title": "Homework 3: Image Classification",
    "section": "1. Load Packages and Obtain Data",
    "text": "1. Load Packages and Obtain Data\nStart by making a code block in which you’ll hold your import statements. You can update this block as you go. For now, include\nimport os\nfrom tensorflow.keras import utils \nNow, let’s access the data. We’ll use a sample data set provided by the TensorFlow team that contains labeled images of cats and dogs.\nPaste and run the following code block.\n# location of data\n_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n\n# download the data and extract it\npath_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n\n# construct paths\nPATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n\ntrain_dir = os.path.join(PATH, 'train')\nvalidation_dir = os.path.join(PATH, 'validation')\n\n# parameters for datasets\nBATCH_SIZE = 32\nIMG_SIZE = (160, 160)\n\n# construct train and validation datasets \ntrain_dataset = utils.image_dataset_from_directory(train_dir,\n                                                   shuffle=True,\n                                                   batch_size=BATCH_SIZE,\n                                                   image_size=IMG_SIZE)\n\nvalidation_dataset = utils.image_dataset_from_directory(validation_dir,\n                                                        shuffle=True,\n                                                        batch_size=BATCH_SIZE,\n                                                        image_size=IMG_SIZE)\n\n# construct the test dataset by taking every 5th observation out of the validation dataset\nval_batches = tf.data.experimental.cardinality(validation_dataset)\ntest_dataset = validation_dataset.take(val_batches // 5)\nvalidation_dataset = validation_dataset.skip(val_batches // 5)\nBy running this code, we have created TensorFlow Datasets for training, validation, and testing. You can think of a Dataset as a pipeline that feeds data to a machine learning model. We use data sets in cases in which it’s not necessarily practical to load all the data into memory.\nIn our case, we’ve used a special-purpose keras utility called image_dataset_from_directory to construct a Dataset. The most important argument is the first one, which says where the images are located. The shuffle argument says that, when retrieving data from this directory, the order should be randomized. The batch_size determines how many data points are gathered from the directory at once. Here, for example, each time we request some data we will get 32 images from each of the data sets. Finally, the image_size specifies the size of the input images, just like you’d expect.\nPaste the following code into the next block. This is technical code related to rapidly reading data. If you’re interested in learning more about this kind of thing, you can take a look here.\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n\nWorking with Datasets\nYou can get a piece of a data set using the take method; e.g. train_dataset.take(1) will retrieve one batch (32 images with labels) from the training data.\nLet’s briefly explore our data set. Write a function to create a two-row visualization. In the first row, show three random pictures of cats. In the second row, show three random pictures of dogs. You can see some related code in the linked tutorial above, although you’ll need to make some modifications in order to separate cats and dogs by rows. A docstring is not required.\n\n\nCheck Label Frequencies\nThe following line of code will create an iterator called labels_iterator.\nlabels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()\nCompute the number of images in the training data with label 0 (corresponding to \"cat\") and label 1 (corresponding to \"dog\").\nThe baseline machine learning model is the model that always guesses the most frequent label. Briefly discuss how accurate the baseline model would be in our case.\nWe’ll treat this as the benchmark for improvement. Our models should do much better than baseline in order to be considered good data science achievements!"
  },
  {
    "objectID": "posts/hw3/index.html#first-model",
    "href": "posts/hw3/index.html#first-model",
    "title": "Homework 3: Image Classification",
    "section": "2. First Model",
    "text": "2. First Model\nCreate a tf.keras.Sequential model using some of the layers we’ve discussed in class. In each model, include at least two Conv2D layers, at least two MaxPooling2D layers, at least one Flatten layer, at least one Dense layer, and at least one Dropout layer. Train your model and plot the history of the accuracy on both the training and validation sets. Give your model the name model1.\nTo train a model on a Dataset, use syntax like this:\nhistory = model1.fit(train_dataset, \n                     epochs=20, \n                     validation_data=validation\nHere and in later parts of this assignment, training for 20 epochs with the Dataset settings described above should be sufficient.\nYou don’t have to show multiple models, but please do a few experiments to try to get the best validation accuracy you can. Briefly describe a few of the things you tried. Please make sure that you are able to consistently achieve at least 52% validation accuracy in this part (i.e. just a bit better than baseline).\n\nIn bold font, describe the validation accuracy of your model during training. You don’t have to be precise. For example, “the accuracy of my model stabilized between 65% and 70% during training.”\nThen, compare that to the baseline. How much better did you do?\nOverfitting can be observed when the training accuracy is much higher than the validation accuracy. Do you observe overfitting in model1?"
  },
  {
    "objectID": "posts/hw3/index.html#model-with-data-augmentation",
    "href": "posts/hw3/index.html#model-with-data-augmentation",
    "title": "Homework 3: Image Classification",
    "section": "3. Model with Data Augmentation",
    "text": "3. Model with Data Augmentation\nNow we’re going to add some data augmentation layers to your model. Data augmentation refers to the practice of including modified copies of the same image in the training set. For example, a picture of a cat is still a picture of a cat even if we flip it upside down or rotate it 90 degrees. We can include such transformed versions of the image in our training process in order to help our model learn so-called invariant features of our input images.\n\nFirst, create a tf.keras.layers.RandomFlip() layer. Make a plot of the original image and a few copies to which RandomFlip() has been applied. Make sure to check the documentation for this function!\nNext, create a tf.keras.layers.RandomRotation() layer. Check the docs to learn more about the arguments accepted by this layer. Then, make a plot of both the original image and a few copies to which RandomRotation() has been applied. Now, create a new tf.keras.models.Sequential model called model2 in which the first two layers are augmentation layers. Use a RandomFlip() layer and a RandomRotation() layer. Train your model, and visualize the training history.\n\nPlease make sure that you are able to consistently achieve at least 55% validation accuracy in this part. Scores of near 60% are possible.\nNote: You might find that your model in this section performs a bit worse than the one before, even on the validation set. If so, just comment on it! That doesn’t mean there’s anything wrong with your approach. We’ll see improvements soon.\n\nIn bold font, describe the validation accuracy of your model during training.\nComment on this validation accuracy in comparison to the accuracy you were able to obtain with model1.\nComment again on overfitting. Do you observe overfitting in model2?"
  },
  {
    "objectID": "posts/hw3/index.html#data-preprocessing",
    "href": "posts/hw3/index.html#data-preprocessing",
    "title": "Homework 3: Image Classification",
    "section": "4. Data Preprocessing",
    "text": "4. Data Preprocessing\nSometimes, it can be helpful to make simple transformations to the input data. For example, in this case, the original data has pixels with RGB values between 0 and 255, but many models will train faster with RGB values normalized between 0 and 1, or possibly between -1 and 1. These are mathematically identical situations, since we can always just scale the weights. But if we handle the scaling prior to the training process, we can spend more of our training energy handling actual signal in the data and less energy having the weights adjust to the data scale.\nThe following code will create a preprocessing layer called preprocessor which you can slot into your model pipeline.\ni = tf.keras.Input(shape=(160, 160, 3))\nx = tf.keras.applications.mobilenet_v2.preprocess_input(i)\npreprocessor = tf.keras.Model(inputs = [i], outputs = [x])\nI suggest incorporating the preprocessor layer as the very first layer, before the data augmentation layers. Call the resulting model model3.\nNow, train this model and visualize the training history. This time, please make sure that you are able to achieve at least 70% validation accuracy.\n\nIn bold font, describe the validation accuracy of your model during training.\nComment on this validation accuracy in comparison to the accuracy you were able to obtain with model1.\nComment again on overfitting. Do you observe overfitting in model3?"
  },
  {
    "objectID": "posts/hw3/index.html#transfer-learning",
    "href": "posts/hw3/index.html#transfer-learning",
    "title": "Homework 3: Image Classification",
    "section": "5. Transfer Learning",
    "text": "5. Transfer Learning\nSo far, we’ve been training models for distinguishing between cats and dogs from scratch. In some cases, however, someone might already have trained a model that does a related task, and might have learned some relevant patterns. For example, folks train machine learning models for a variety of image recognition tasks. Maybe we could use a pre-existing model for our task?\nTo do this, we need to first access a pre-existing “base model”, incorporate it into a full model for our current task, and then train that model.\nPaste the following code in order to download MobileNetV2 and configure it as a layer that can be included in your model.\nIMG_SHAPE = IMG_SIZE + (3,)\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = tf.keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = tf.keras.Model(inputs = [i], outputs = [x])\nNow, create a model called model4 that uses MobileNetV2. For this, you should definitely use the following layers:\n\nThe preprocessor layer from Part §4.\nThe data augmentation layers from Part §3.\nThe base_model_layer constructed above.\nA Dense(2) layer at the very end to actually perform the classification.\n\nBetween 3. and 4., you might want to place a small number of additional layers, like GlobalMaxPooling2D or possibly Dropout. You don’t need a lot though! Once you’ve constructed the model, check the model.summary() to see why – there is a LOT of complexity hidden in the base_model_layer. Show the summary and comment. How many parameters do we have to train in the model?\nFinally, train your model for 20 epochs, and visualize the training history.\nThis time, please make sure that you are able to achieve at least 95% validation accuracy. That’s not a typo!\n\nIn bold font, describe the validation accuracy of your model during training.\nComment on this validation accuracy in comparison to the accuracy you were able to obtain with model4.\nComment again on overfitting. Do you observe overfitting in model4?"
  },
  {
    "objectID": "posts/hw3/index.html#score-on-test-data",
    "href": "posts/hw3/index.html#score-on-test-data",
    "title": "Homework 3: Image Classification",
    "section": "6. Score on Test Data",
    "text": "6. Score on Test Data\nFeel free to mess around with various model structures and settings in order to get the best validation accuracy you can. Finally, evaluate the accuracy of your most performant model on the unseen test_dataset. How’d you do?"
  },
  {
    "objectID": "posts/hw3/index.html#write-your-blog-post",
    "href": "posts/hw3/index.html#write-your-blog-post",
    "title": "Homework 3: Image Classification",
    "section": "7. Write Your Blog Post",
    "text": "7. Write Your Blog Post\nTurn your work into a tutorial Blog Post on the topic of image classification and transfer learning. Make sure to include all the code, summaries, and plots that you created. You might find it useful to refer to the Keras tutorial linked above, but your entire post should be in your own original words. Feel free to link your reader to any resources that you found helpful."
  },
  {
    "objectID": "posts/hw3/index.html#format",
    "href": "posts/hw3/index.html#format",
    "title": "Homework 3: Image Classification",
    "section": "Format",
    "text": "Format\n\nAs always, please submit the PDF printout of your blog post and any code you wrote."
  },
  {
    "objectID": "posts/hw3/index.html#coding-problem",
    "href": "posts/hw3/index.html#coding-problem",
    "title": "Homework 3: Image Classification",
    "section": "Coding Problem",
    "text": "Coding Problem\n\nThe visualization in Part 1 is implemented using a function and shows labeled images of cats in one row and labeled images of dogs in another row.\nThere are two visualizations in Part 3 showing the results of applying RandomFlip and RandomRotation to an example image.\nModels 1, 2, 3, and 4 are logically constructed and obtain the required validation accuracy in each case:\n\nModel 1 should obtain at least 52% validation accuracy.\nModel 2 should obtain at least 55% validation accuracy.\nModel 3 should obtain at least 70% validation accuracy.\nModel 4 should obtain at least 95% validation accuracy.\n\nThe training history is shown for each of the four models, including the training and validation performance.\nThe most performant model is evaluated on the test data set."
  },
  {
    "objectID": "posts/hw3/index.html#style-and-documentation",
    "href": "posts/hw3/index.html#style-and-documentation",
    "title": "Homework 3: Image Classification",
    "section": "Style and Documentation",
    "text": "Style and Documentation\n\nCode throughout is written using minimal repetition and clean style.\nDocstrings are not required in this Blog Post, but please make sure to include useful comments and detailed explanations for each of your code blocks."
  },
  {
    "objectID": "posts/hw3/index.html#writing",
    "href": "posts/hw3/index.html#writing",
    "title": "Homework 3: Image Classification",
    "section": "Writing",
    "text": "Writing\n\nThe blog post is written in tutorial format, in engaging and clear English. Grammar and spelling errors are acceptable within reason."
  },
  {
    "objectID": "posts/hw1/index.html",
    "href": "posts/hw1/index.html",
    "title": "Homework 1: Data Wrangling and Visualization",
    "section": "",
    "text": "In this blog post, you’ll create several interesting, interactive data graphics using the NOAA climate data that we’ve explored in the first several weeks of lectures."
  },
  {
    "objectID": "posts/hw1/index.html#instructions",
    "href": "posts/hw1/index.html#instructions",
    "title": "Homework 1: Data Wrangling and Visualization",
    "section": "Instructions",
    "text": "Instructions\nYour post should include not only code but also outputs (i.e. tables, figures) and expository writing that explains what you’re doing. Your target audience is a a student who has completed PIC16A but hasn’t taken PIC16B yet (i.e. you before the start of the quarter).\nSee the “Specifications” section at the bottom for a detailed list of specs."
  },
  {
    "objectID": "posts/hw1/index.html#create-a-database",
    "href": "posts/hw1/index.html#create-a-database",
    "title": "Homework 1: Data Wrangling and Visualization",
    "section": "1. Create a Database",
    "text": "1. Create a Database\nFirst, create a database with three tables: temperatures, stations, and countries. Refer back to lecture notes on how to access country names and relate them to temperature readings. Keep these as three separate tables in your database.\nMake sure to close the database connection after you are finished constructing it."
  },
  {
    "objectID": "posts/hw1/index.html#write-a-query-function",
    "href": "posts/hw1/index.html#write-a-query-function",
    "title": "Homework 1: Data Wrangling and Visualization",
    "section": "2. Write a Query Function",
    "text": "2. Write a Query Function\nWrite a function called query_climate_database() which accepts four arguments:\n\ncountry, a string giving the name of a country for which data should be returned.\nyear_begin and year_end, two integers giving the earliest and latest years for which should be returned.\nmonth, an integer giving the month of the year for which should be returned.\n\nThe return value of query_climate_database() is a Pandas dataframe of temperature readings for the specified country, in the specified date range, in the specified month of the year. This dataframe should have columns for:\n\nThe station name.\nThe latitude of the station.\nThe longitude of the station.\nThe name of the country in which the station is located.\nThe year in which the reading was taken.\nThe month in which the reading was taken.\nThe average temperature at the specified station during the specified year and month. (Note: the temperatures in the raw data are already averages by month, so you don’t have to do any aggregation at this stage.)\n\nHint: Inside the function, you would want to put in the given variables (like year_begin) inside the query string (like SELECT ... FROM ... WHERE ...). While you can do this with string concatenation, using the fancy Python f-strings will probably make your life easier. Be careful not to forget quotation marks for the string variables.\nFor example:\nquery_climate_database(country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\n\nNAME\n\n\nLATITUDE\n\n\nLONGITUDE\n\n\nCountry\n\n\nYear\n\n\nMonth\n\n\nTemp\n\n\n\n\n\n\n0\n\n\nPBO_ANANTAPUR\n\n\n14.583\n\n\n77.633\n\n\nIndia\n\n\n1980\n\n\n1\n\n\n23.48\n\n\n\n\n1\n\n\nPBO_ANANTAPUR\n\n\n14.583\n\n\n77.633\n\n\nIndia\n\n\n1981\n\n\n1\n\n\n24.57\n\n\n\n\n2\n\n\nPBO_ANANTAPUR\n\n\n14.583\n\n\n77.633\n\n\nIndia\n\n\n1982\n\n\n1\n\n\n24.19\n\n\n\n\n3\n\n\nPBO_ANANTAPUR\n\n\n14.583\n\n\n77.633\n\n\nIndia\n\n\n1983\n\n\n1\n\n\n23.51\n\n\n\n\n4\n\n\nPBO_ANANTAPUR\n\n\n14.583\n\n\n77.633\n\n\nIndia\n\n\n1984\n\n\n1\n\n\n24.81\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n3147\n\n\nDARJEELING\n\n\n27.050\n\n\n88.270\n\n\nIndia\n\n\n1983\n\n\n1\n\n\n5.10\n\n\n\n\n3148\n\n\nDARJEELING\n\n\n27.050\n\n\n88.270\n\n\nIndia\n\n\n1986\n\n\n1\n\n\n6.90\n\n\n\n\n3149\n\n\nDARJEELING\n\n\n27.050\n\n\n88.270\n\n\nIndia\n\n\n1994\n\n\n1\n\n\n8.10\n\n\n\n\n3150\n\n\nDARJEELING\n\n\n27.050\n\n\n88.270\n\n\nIndia\n\n\n1995\n\n\n1\n\n\n5.60\n\n\n\n\n3151\n\n\nDARJEELING\n\n\n27.050\n\n\n88.270\n\n\nIndia\n\n\n1997\n\n\n1\n\n\n5.70\n\n\n\n\n\n\n3152 rows × 7 columns"
  },
  {
    "objectID": "posts/hw1/index.html#write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "href": "posts/hw1/index.html#write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "title": "Homework 1: Data Wrangling and Visualization",
    "section": "3. Write a Geographic Scatter Function for Yearly Temperature Increases",
    "text": "3. Write a Geographic Scatter Function for Yearly Temperature Increases\nIn this part, you will write a function to create visualizations that address the following question:\n\nHow does the average yearly change in temperature vary within a given country?\n\nWrite a function called temperature_coefficient_plot(). This function should accept five explicit arguments, and an undetermined number of keyword arguments.\n\ncountry, year_begin, year_end, and month should be as in the previous part.\nmin_obs, the minimum required number of years of data for any given station. Only data for stations with at least min_obs years worth of data in the specified month should be plotted; the others should be filtered out. df.transform() plus filtering is a good way to achieve this task.\n**kwargs, additional keyword arguments passed to px.scatter_mapbox(). These can be used to control the colormap used, the mapbox style, etc.\n\nThe output of this function should be an interactive geographic scatterplot, constructed using Plotly Express, with a point for each station, such that the color of the point reflects an estimate of the yearly change in temperature during the specified month and time period at that station. A reasonable way to do this is to compute the first coefficient of a linear regression model at that station, as illustrated in the lecture where we used the .apply() method.\nFor example, after writing your function, you should be able to create a plot of estimated yearly increases in temperature during the month of January, in the interval 1980-2020, in India, as follows:\n# assumes you have imported necessary packages\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\nPlease pay attention to the following details:\n\nThe station name is shown when you hover over the corresponding point on the map.\nThe estimates shown in the hover are rounded to a sober number of significant figures.\nThe colorbar and overall plot have professional titles.\nThe colorbar is centered at 0, so that the “middle” of the colorbar (white, in this case) corresponds to a coefficient of 0.\n\nIt’s not necessary for your plot to look exactly like mine, but please attend to details such as these. Feel free to be creative about these labels, as well as the choice of colors, as long as your result is polished overall.\nYou are free (and indeed encouraged) to define additional functions as needed."
  },
  {
    "objectID": "posts/hw1/index.html#create-two-more-interesting-figures",
    "href": "posts/hw1/index.html#create-two-more-interesting-figures",
    "title": "Homework 1: Data Wrangling and Visualization",
    "section": "4. Create Two More Interesting Figures",
    "text": "4. Create Two More Interesting Figures\nCreate at least one more SQL query function and at least two more complex and interesting interactive data visualizations using the same data set. These plots must be of different types (e.g. line and bar, scatter and histogram, etc). The code to construct each visualization should be wrapped in functions, such that a user could create visualizations for different parts of the data by calling these functions with different arguments. At least one of these plots must involve multiple facets (i.e. multiple axes (in the sense of facets), each of which shows a subset of the data).\nAlongside the plots, you should clearly state a question that the plot addresses, similar to the question that we posed in Part 3. The questions for your two additional plots should be meaningfully different from each other and from the Part 3 question. You will likely want to define different query functions for extracting data for these new visualizations.\nIt is not necessary to create geographic plots for this part. Scatterplots, histograms, and line plots (among other choices) are all appropriate. Please make sure that they are complex, engaging, professional, and targeted to the questions you posed. In other words, push yourself! Don’t hesitate to ask your peers or talk to me if you’re having trouble coming up with questions or identifying plots that might be suitable for addressing those questions.\nThere will be two Gradescope assignments open for submission, one for PDF, and the other for code portion (as a programming assignment). You have to submit both of them for your homework to be graded.\n\nFor the PDF assingment, please submit your blog page printed as PDF. You do not need to publish your blog to the web for the homework. It is enough to print the preview page as a PDF and submit on Gradescope. If you want to publish it online, you are welcome to do so. However, please make sure your code is visible in full, i.e., not cuttoff, in your pdf.\nFor the programming assignment, please submit any code file you wrote and any HTML files generated within your post directory, but not the data files. All the .py file, .ipynb file, .qmd, .html files are included, but .csv files or .db files are not. The grader should be able to reproduce your result from the code portion you submitted."
  },
  {
    "objectID": "posts/hw1/index.html#tip",
    "href": "posts/hw1/index.html#tip",
    "title": "Homework 1: Data Wrangling and Visualization",
    "section": "Tip",
    "text": "Tip\nTo properly show figures in your blog, you may need to set your plotly renderer to iframe. To do so, run the following near the top of your notebook:\nimport plotly.io as pio\npio.renderers.default=\"iframe\""
  },
  {
    "objectID": "posts/hw1/index.html#format",
    "href": "posts/hw1/index.html#format",
    "title": "Homework 1: Data Wrangling and Visualization",
    "section": "Format",
    "text": "Format\n\nYou have to submit the PDF-printed version of your Quarto blog and any code necessary to reproduce your results.\n\n\nAlso, you must submit any figures generated in the HTML format."
  },
  {
    "objectID": "posts/hw1/index.html#coding-problem",
    "href": "posts/hw1/index.html#coding-problem",
    "title": "Homework 1: Data Wrangling and Visualization",
    "section": "Coding Problem",
    "text": "Coding Problem\n\nThe query_climate_database() function is correctly defined according to the prompt.\nThere are two geographic scatterplots, one for 1980-2020 India that looks similar to the provided example, and another one for a different time and/or country.\nThe geographic scatterplots are correctly constructed and professionally labeled, including a title, hovers with rounded estimates, and a correctly centered colorbar.\nThere is at least one more sql query function defined.\nThere are two other interactive plots constructed using Plotly.\nOne of these plots involves the use of multiple facets.\nThe two other plots are wrapped in appropriate, user-friendly functions.\nEach of the two other interactive plots have descriptive titles and centered color maps when appropriate."
  },
  {
    "objectID": "posts/hw1/index.html#style-and-documentation",
    "href": "posts/hw1/index.html#style-and-documentation",
    "title": "Homework 1: Data Wrangling and Visualization",
    "section": "Style and Documentation",
    "text": "Style and Documentation\n\nRepeated operations should are enclosed in functions.\nFor-loops are minimized by making full use of vectorized operations for Numpy arrays and Pandas data frames.\nHelpful comments are supplied throughout the code. Docstrings are supplied for any functions and classes you define."
  },
  {
    "objectID": "posts/hw1/index.html#writing",
    "href": "posts/hw1/index.html#writing",
    "title": "Homework 1: Data Wrangling and Visualization",
    "section": "Writing",
    "text": "Writing\n\nThe overall post is written in engaging and unambiguous English prose. There is written explanation throughout the post, such that a PIC16A student could learn to perform the demonstrated tasks by reading the post.\n\nEach block of code has a clearly-explained purpose.\nThe post is organized into clearly delimited sections using markdown headers (#), making it easier for the reader to navigate.\n\nThe post has a different title from “Blog Post: Data Wrangling and Visualization” or “Blog Post 1”. Please rename it to something more relevant and specific to your data analysis."
  },
  {
    "objectID": "posts/composing/index.html",
    "href": "posts/composing/index.html",
    "title": "Creating Posts",
    "section": "",
    "text": "How to create technical posts that include Python code, explanatory text, and notes."
  },
  {
    "objectID": "posts/composing/index.html#directory",
    "href": "posts/composing/index.html#directory",
    "title": "Creating Posts",
    "section": "Directory",
    "text": "Directory\nYour posts should be placed in the posts/ directory of your website.\nIf you want to make a new page called bruin, then create a new folder named bruin/ inside posts/. For example:\nposts\n└───composing \n└───bruin &lt; new folder\n└───quarto\n└───software\n└───welcome"
  },
  {
    "objectID": "posts/composing/index.html#create-the-file",
    "href": "posts/composing/index.html#create-the-file",
    "title": "Creating Posts",
    "section": "Create the File",
    "text": "Create the File\nYou have two options with the folder bruin/.\nOption 1: Add a Jupyter Notebook named index.ipynb Since your homework posts will be based on previous work you did in a Jupyter notebook or Google colab, this will probably be the easier option for publishing homeworks.\nOption 2: Add a index.qmd text file But this is probably a better option for your group project blog post, and once you (hopefully) continue to build up your portfolio using this website.\nFor either options, make sure to add a header that looks like this to the top:\n---\ntitle: \"Creating posts\"\nauthor: \"Seyoon\"\ndate: \"2022-12-24\"\ncategories: [week 0, example]\n---\nIn Jupyter notebook, this header should be in a raw cell up top."
  },
  {
    "objectID": "posts/composing/index.html#markdown-styling",
    "href": "posts/composing/index.html#markdown-styling",
    "title": "Creating Posts",
    "section": "Markdown Styling",
    "text": "Markdown Styling\nYou can use Markdown to style basic text, much as you do in Jupyter Notebooks.\nLook into Quarto’s Markdown basics, Figures and Tables. You’re welcome to explore other pages that cover more complex concepts like Diagrams, Videos, and Callout Blocks."
  },
  {
    "objectID": "posts/composing/index.html#math",
    "href": "posts/composing/index.html#math",
    "title": "Creating Posts",
    "section": "Math",
    "text": "Math\nIf you are familiar with the \\[\\LaTeX\\] typesetting system, you can use many standard commands by enclosing them in double $ symbols. You can make both inline math like \\[\nf(x) = e^x\n\\] and display math like \\[\n\\sum_{i=1}^\\infty \\frac{1}{i^2} = \\frac{\\pi^2}{6}.\n\\]"
  },
  {
    "objectID": "posts/composing/index.html#images",
    "href": "posts/composing/index.html#images",
    "title": "Creating Posts",
    "section": "Images",
    "text": "Images\nYou can and should include images in your posts, especially in cases where you have created a data visualization. If the image is already available online, you can link to it using the syntax ![alt text](image_url):\n\n(Source: https://xkcd.com/353/)"
  },
  {
    "objectID": "posts/composing/index.html#code",
    "href": "posts/composing/index.html#code",
    "title": "Creating Posts",
    "section": "Code",
    "text": "Code\nThere are two main ways to insert code in your posts. When talking about a short concept, like the np.random.rand() function, you can type back ticks like this: `np.random.rand()`.\nTo create a larger block of code, use three consecutive backticks ``` to both open and close the code block. If you place the word “{python}” immediately after the opening code blocks, you’ll get attractive syntax highlighting:\n\ndef f(x):\n    \"\"\"\n    A cool function that multiples an input x by 2. \n    \"\"\"\n    return 2*x\n\ny = f(3)\nprint(y)\n\n6\n\n\nNot only that, once you render the page with Quarto, the code output will show up below. If that’s not what you want, use the word “python” instead of “{python}”\nLook at this other cool example from the Quarto tutorial.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PIC16B-23F",
    "section": "",
    "text": "Homework 4: Fake News Classification\n\n\n\n\n\n\n\nHomework\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHomework 3: Image Classification\n\n\n\n\n\n\n\nHomework\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHomework 2: Web Scraping\n\n\n\n\n\n\n\nHomework\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHomework 1: Data Wrangling and Visualization\n\n\n\n\n\n\n\nHomework\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHomework 0\n\n\n\n\n\n\n\nHomework\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nCreating Posts\n\n\n\n\n\n\n\nWeek 0\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHello, Quarto\n\n\n\n\n\n\n\nWeek 0\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\n\n\n\n\nNo matching items"
  }
]